import json
import os
import re
import subprocess
import tempfile
import time

import google.generativeai as genai
from dotenv import load_dotenv

# ========= í™˜ê²½ ì„¸íŒ… =========
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

# ëª¨ë¸/ì„¤ì • í™˜ê²½ë³€ìˆ˜ (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’)
IMPL = os.getenv("WHISPER_IMPL", "faster")                 # "faster" or "whisper"
FASTER_MODEL = os.getenv("FASTER_WHISPER_MODEL", "tiny")   # tiny|base|small|medium...
WHISPER_MODEL = os.getenv("WHISPER_MODEL", "tiny")         # í´ë°± whisper
DEVICE = os.getenv("WHISPER_DEVICE", "cpu")                # PiëŠ” cpu
COMPUTE_TYPE = os.getenv("FASTER_COMPUTE_TYPE", "int8")    # int8|int8_float16|auto
CPU_THREADS = int(os.getenv("CPU_THREADS", "4"))           # Pi5ëŠ” 4 ê¶Œì¥

# ========= Gemini (JSON ê°•ì œ) =========
genai.configure(api_key=GOOGLE_API_KEY)
generation_config = {
    "temperature": 0.3,
    "max_output_tokens": 1024,
    "response_mime_type": "application/json",
}
gemini_model = genai.GenerativeModel(
    model_name="gemini-2.0-flash-001",
    generation_config=generation_config
)

TARGET_KEYS = ["feedback", "confidence", "tone", "resilience", "good", "bad"]

# ========= ê³µí†µ ìœ í‹¸ =========
def _coerce_int(v, default=0, lo=0, hi=100):
    try:
        x = int(round(float(v)))
        return max(lo, min(hi, x))
    except:
        return default

def _extract_json(text: str) -> str:
    s = (text or "").strip()
    if s.startswith("```"):
        s = re.sub(r"^```(?:json)?\s*", "", s)
        s = re.sub(r"\s*```$", "", s).strip()
    i, j = s.find("{"), s.rfind("}")
    return s[i:j+1] if i != -1 and j != -1 and j > i else s

def _ensure_feedback_schema(d: dict) -> dict:
    """ì„ì˜ ìŠ¤í‚¤ë§ˆ â†’ ìš°ë¦¬ê°€ ì›í•˜ëŠ” 6í‚¤ë¡œ ì •ê·œí™”"""
    # overallë¥˜ â†’ feedback
    feedback = (
        d.get("feedback") or
        d.get("overall_feedback") or
        d.get("overall") or
        d.get("summary") or
        ""
    )
    feedback = str(feedback).strip() or "í•µì‹¬ ì‚¬ë¡€ë¥¼ ë¨¼ì € ì œì‹œí•˜ë©´ ë” íš¨ê³¼ì ì…ë‹ˆë‹¤."

    # good/bad í›„ë³´
    good = (d.get("good") or "").strip()
    bad  = (d.get("bad")  or "").strip()
    pos = d.get("positive_aspects")
    if not good and isinstance(pos, dict):
        good = " ".join([str(v) for v in pos.values() if v]).strip()
    if not good and isinstance(pos, list):
        good = " ".join([str(x) for x in pos if x]).strip()
    neg = d.get("areas_for_improvement") or d.get("specific_feedback")
    if not bad and isinstance(neg, dict):
        bad = " ".join([str(v) for v in neg.values() if v]).strip()
    if not bad and isinstance(neg, list):
        bad = " ".join([str(x) for x in neg if x]).strip()
    if not good:
        good = "êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ì˜ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤."
    if not bad:
        bad = "ë‹µë³€ì´ ë‹¤ì†Œ ì¥í™©í–ˆìŠµë‹ˆë‹¤."

    # ìˆ˜ì¹˜
    confidence = _coerce_int(d.get("confidence", d.get("overall_score", 65)), 65)
    tone = _coerce_int(d.get("tone", 70), 70)
    resilience = _coerce_int(d.get("resilience", 70), 70)

    out = {
        "feedback": feedback,
        "confidence": confidence,
        "tone": tone,
        "resilience": resilience,
        "good": good,
        "bad": bad,
    }
    return {k: out[k] for k in TARGET_KEYS}

def _extract_audio_to_wav16(video_path: str, strip_silence: bool = True) -> str:
    """
    ffmpegë¡œ 16kHz mono wav ì¶”ì¶œ (+ì˜µì…˜: ë¬´ìŒ êµ¬ê°„ ì œê±°)
    strip_silence=Trueë©´ ë¬´ì„± êµ¬ê°„ì„ ì œê±°í•˜ì—¬ ê¸¸ì´ ì¶•ì†Œ â†’ ì†ë„â†‘
    """
    try:
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".wav")
        tmp.close()
        cmd = [
            "ffmpeg", "-y", "-i", video_path, "-vn",
            "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1"
        ]
        if strip_silence:
            cmd += ["-af", "silenceremove=start_periods=1:start_duration=0.5:start_threshold=-40dB:"
                            "stop_periods=1:stop_duration=0.5:stop_threshold=-40dB"]
        cmd += [tmp.name]
        subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=True)
        return tmp.name
    except Exception:
        return video_path

# ========= ASR: faster-whisper ìš°ì„ , whisper í´ë°± =========
_faster_model = None
_whisper_model = None
_faster_available = False

def _init_asr_models():
    global _faster_model, _whisper_model, _faster_available

    if IMPL.lower() == "faster":
        try:
            from faster_whisper import WhisperModel  # lazy import
            print(f"faster-whisper ëª¨ë¸ ë¡œë”© ì¤‘... (model={FASTER_MODEL}, device=cpu, compute_type={COMPUTE_TYPE}, threads={CPU_THREADS})")
            _faster_model = WhisperModel(
                FASTER_MODEL, device="cpu", compute_type=COMPUTE_TYPE, cpu_threads=CPU_THREADS
            )
            _faster_available = True
            print("âœ… faster-whisper ë¡œë”© ì™„ë£Œ")
            return
        except Exception as e:
            print(f"âš ï¸ faster-whisper ì‚¬ìš© ë¶ˆê°€, whisperë¡œ í´ë°±í•©ë‹ˆë‹¤: {e}")

    # whisper í´ë°±
    try:
        import whisper
        print(f"Whisper ëª¨ë¸ ë¡œë”© ì¤‘... (model={WHISPER_MODEL}, device=cpu)")
        _whisper_model = whisper.load_model(WHISPER_MODEL, device="cpu")
        print("âœ… Whisper ë¡œë”© ì™„ë£Œ")
    except Exception as e:
        print(f"âŒ Whisper ë¡œë”© ì‹¤íŒ¨: {e}")
        raise

_init_asr_models()

def _transcribe_faster_whisper(audio_path: str) -> str:
    """faster-whisper ê³ ì† ë³€í™˜ (VAD on)"""
    assert _faster_model is not None
    start = time.time()
    print("ğŸ§ faster-whisper ë³€í™˜ ì‹œì‘...")

    segments, info = _faster_model.transcribe(
        audio_path,
        language="ko",
        beam_size=1,               # ì†ë„ ìš°ì„ 
        vad_filter=True,           # ë¬´ìŒ ìë™ ìŠ¤í‚µ
        vad_parameters={"min_speech_duration_ms": 250}
    )
    text_parts = []
    for seg in segments:
        text_parts.append(seg.text)
    out = " ".join(text_parts).strip()

    elapsed = round(time.time() - start, 2)
    print(f"âœ… faster-whisper ë³€í™˜ ì™„ë£Œ ({elapsed}ì´ˆ)")
    return out

def _transcribe_whisper(audio_path: str) -> str:
    """ì›ë³¸ whisper í´ë°± (CPUë©´ ëŠë¦´ ìˆ˜ ìˆìŒ)"""
    import whisper
    start = time.time()
    print("ğŸ§ Whisper ë³€í™˜ ì‹œì‘...")
    result = _whisper_model.transcribe(
        audio_path,
        language="ko",
        fp16=False,
        beam_size=1,
        best_of=1,
        word_timestamps=False,
        condition_on_previous_text=False,
        no_speech_threshold=0.6
    )
    elapsed = round(time.time() - start, 2)
    print(f"âœ… Whisper ë³€í™˜ ì™„ë£Œ ({elapsed}ì´ˆ)")
    return result.get("text", "").strip()

def transcribe_audio_with_asr(audio_or_video_path: str) -> str:
    """ë¹„ë””ì˜¤ â†’ ì˜¤ë””ì˜¤ ì¶”ì¶œ(ë¬´ìŒ ì œê±°) â†’ faster-whisper or whisper"""
    path = _extract_audio_to_wav16(audio_or_video_path, strip_silence=True)
    try:
        if _faster_available:
            return _transcribe_faster_whisper(path)
        else:
            return _transcribe_whisper(path)
    finally:
        if path != audio_or_video_path and os.path.exists(path):
            os.unlink(path)

# ========= Gemini í˜¸ì¶œ =========
def generate_feedback_with_gemini(question, answer):
    print("ğŸ§  Gemini í”¼ë“œë°± ìƒì„± ì¤‘...")
    start = time.time()

    prompt = f"""
ì•„ë˜ ë©´ì ‘ ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”.
ë°˜ë“œì‹œ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. 
í‚¤ëŠ” ë‹¤ìŒ 6ê°œë§Œ í¬í•¨í•©ë‹ˆë‹¤: feedback, confidence, tone, resilience, good, bad.

ë©´ì ‘ ì§ˆë¬¸: "{question}"
ë©´ì ‘ì ë‹µë³€: "{answer}"

ì¶œë ¥ í˜•ì‹ ì˜ˆì‹œ:
{{
  "feedback": "í•µì‹¬ ì‚¬ë¡€ë¥¼ ë¨¼ì € ì œì‹œí•˜ë©´ ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.",
  "confidence": 75,
  "tone": 68,
  "resilience": 80,
  "good": "êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ì˜ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.",
  "bad": "ë‹µë³€ì´ ë‹¤ì†Œ ì¥í™©í–ˆìŠµë‹ˆë‹¤."
}}

ì œì•½:
- ì˜¤ì§ ìœ„ 6ê°œ í‚¤ë§Œ í¬í•¨.
- ì½”ë“œ ë¸”ë¡( ``` ) ì¶œë ¥ ê¸ˆì§€.
- JSON ì™¸ í…ìŠ¤íŠ¸ ê¸ˆì§€.
- feedbackì€ ë°˜ë“œì‹œ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½ëœ ì „ì²´ í‰ê°€ ë¬¸ì¥.
"""

    raw = ""
    try:
        res = gemini_model.generate_content(prompt)
        raw = (res.text or "").strip()
        body = _extract_json(raw)
        parsed = json.loads(body)
    except Exception as e:
        print(f"âš ï¸ 1ì°¨ íŒŒì‹± ì‹¤íŒ¨, ë³´ì • ì‹œë„: {e}")
        try:
            body = _extract_json(raw)
            parsed = json.loads(body)
        except Exception as e2:
            print(f"âŒ 2ì°¨ íŒŒì‹± ì‹¤íŒ¨: {e2}")
            elapsed = round(time.time() - start, 2)
            print(f"âœ… Gemini ê¸°ë³¸ê°’ ë°˜í™˜ ({elapsed}ì´ˆ)")
            return {
                "feedback": "í•µì‹¬ ì‚¬ë¡€ë¥¼ ë¨¼ì € ì œì‹œí•˜ë©´ ë” íš¨ê³¼ì ì…ë‹ˆë‹¤.",
                "confidence": 65,
                "tone": 70,
                "resilience": 70,
                "good": "êµ¬ì²´ì ì¸ ì‚¬ë¡€ë¥¼ ì˜ ì„¤ëª…í–ˆìŠµë‹ˆë‹¤.",
                "bad": "ë‹µë³€ì´ ë‹¤ì†Œ ì¥í™©í–ˆìŠµë‹ˆë‹¤."
            }

    final_dict = _ensure_feedback_schema(parsed)
    elapsed = round(time.time() - start, 2)
    print(f"âœ… Gemini í”¼ë“œë°± ì™„ë£Œ ({elapsed}ì´ˆ) -> {final_dict}")
    return final_dict

def run_interview_feedback_service(audio_or_video_path, interview_question):
    text = transcribe_audio_with_asr(audio_or_video_path)
    if not text:
        return {
            "feedback": "ìŒì„± ë³€í™˜ ì‹¤íŒ¨",
            "confidence": 0,
            "tone": 0,
            "resilience": 0,
            "good": "",
            "bad": ""
        }
    print(f"ğŸ—£ï¸ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ì¼ë¶€: {text[:100]}...")
    return generate_feedback_with_gemini(interview_question, text)
